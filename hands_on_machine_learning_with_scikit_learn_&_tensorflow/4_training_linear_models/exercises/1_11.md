### 如果训练集有数百万个特征，可以使用哪种线性回归训练算法?

### 如果训练集里特征的数值大小迥异，哪种算法可能会受到影响？受影响的程度如何？你应该怎么做？

### 训练逻辑回归模型时，梯度下降会卡在局部最小值中吗？

### 如果你让它们运行足够长的时间，是否所有梯度下降算法都能得出相同的模型？

### 假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，可能是什么情况？你该如何解决？

### 当验证错误上升时立即停止小批量梯度下降是个好主意吗？

### 哪种梯度下降算法将最快达到最佳解附近？哪个实际上会收敛？如何使其他的也收敛？

### 假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？

### 假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？

### 为什么要使用：
#### 岭回归而不是简单的线性回归？
#### Lasso而不是岭回归？
#### 弹性网络而不是Lasso？

### 假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑分类回归器还是一个Softmax回归分类器？
