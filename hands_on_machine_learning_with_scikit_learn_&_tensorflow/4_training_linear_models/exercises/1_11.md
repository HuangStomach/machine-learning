### 如果训练集有数百万个特征，可以使用哪种线性回归训练算法?

可以使用随机梯度下降或小批量梯度下降。

### 如果训练集里特征的数值大小迥异，哪种算法可能会受到影响？受影响的程度如何？你应该怎么做？

特征尺寸比例不同会导致成本函数呈细长碗的形状，梯度下降算法会多花费时间。应该提前缩放数据。

标准方程或SVD无须缩放。

如果特征未按比例缩放，则正则化模型可能会收敛至次优解：由于正则化会惩罚较大的权重，因此与具有较大值的特征相比，具有较小值的特征会被忽略。

### 训练逻辑回归模型时，梯度下降会卡在局部最小值中吗？

不会，因为成本函数是凸函数。

### 如果你让它们运行足够长的时间，是否所有梯度下降算法都能得出相同的模型？

凸优化问题学习率不是特别高，那么所有梯度下降算法都会接近全局最优解并得出相似模型。

但是除非逐步降低学习率，否则随机梯度下降和小批量梯度下降将不会真正收敛。

### 假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，可能是什么情况？你该如何解决？

一种可能是学习率过高算法在发散，应该降低学习率。

### 当验证错误上升时立即停止小批量梯度下降是个好主意吗？

由于随机性有可能会短暂产生所错误，所以需要按照一定时间间隔保存模型。很长时间没有改善后再停止。

### 哪种梯度下降算法将最快达到最佳解附近？哪个实际上会收敛？如何使其他的也收敛？

随机梯度下降由于每次只考虑一个训练实例所以很快。但只有批量实际上会收敛，其他算法除非降低学习率，否则只会在最优值附近反弹

### 假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？

可能是过拟合导致，可以降低多项式阶数或者进行正则化。

### 假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？

可能是欠拟合，意味着具有很高的偏差，应该尝试减少超参数

### 为什么要使用：
#### 岭回归而不是简单的线性回归？
#### Lasso而不是岭回归？
#### 弹性网络而不是Lasso？

* 正则化通常效果更好，可以优先考虑岭回归
* Lasso使用L1惩罚，自动进行特征选择，如果怀疑只有很少的特征很重要的情况下优先考虑Lasso
* Lasso在特征可能产生强相关的时候容易产生异常。

### 假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑分类回归器还是一个Softmax回归分类器？

Softmax只是多分类而不是多输出，需要两个逻辑分类回归器。
