### Tensorflow Playground

略

### 使用原始人工神经元绘制一个ANN，以计算 A xor B

略

### 为什么通常最好使用逻辑回归分类器而不是经典的感知器？如何调整感知器使其等同于逻辑回归分类器？

感知器需要实例线性可分。将感知器的激活函数修改为逻辑激活函数，然后使用梯度下降算法来训练，就会变成一个逻辑回归分类器。

### 为什么逻辑激活函数是训练第一个MLP的关键要素

因为导数总是非0的，梯度下降是可以持续的。

### 列举三种常用的激活函数

sigmoid tanh relu

### 假设你有一个MLP，该MLP由一个输入层和十个直通神经元组成，随后是一个包含50个神经元的隐藏层，最后3个神经元组成的输出层。所有人工神经元都使用ReLU激活函数。

* 输入矩阵X的形状：m * 10

* 隐藏层的权重向量Wh及其偏置向量bh的形状：10 * 50 50

* 输出层的权重向量Wo及其偏置向量bo的形状：50 * 3 3

* 网络的输出矩阵Y的形状：m * 3

* 编写等式计算网络的输出矩阵Y Y* = ReLU(ReLU(XWh + bh)Wo + bo)

### 如果要将电子邮件分类为垃圾邮件或者正常邮件，你需要在输出层中有多少个神经元？你应该在输出层中使用什么激活函数？相反如果你想解决MNIST，则在输出层中需要多少个神经元，应该使用哪种激活函数？如第二章所示，如何使你的网络预测房价？

垃圾邮件仅需要一个神经元来体现概率，使用逻辑激活函数。
MNIST问题则需要十个神经元，并且需要使用多分类softmax激活函数
房价问题就只需要一个输出神经元并不需要激活函数

### 什么是反向传播，它如何工作？反向传播和反向模式的autodiff有什么区别？

反向传播首先计算关于每个模型参数的成本函数的梯度，然后使用这些梯度执行梯度下降。
autodiff只是一种实现技术，被反向传播使用了。

### 你能否列出可以在基本MLP中进行调整的所有超参数，如果MLP过拟合训练数据，你如何调整这些超参数来解决问题？

隐藏层数量、神经元数量、每个层的激活函数。
如果过拟合可以考虑减少隐藏层和神经元数量
