### 如果训练集有100万个实例，训练决策树（无约束）的大致深度是多少

二元决策树大多平衡，不进行限制最后会变为每个叶节点一个实例，成为二叉平衡树，决策树深度为log10^6 ~= 20

### 通常来说，子节点的基尼不纯度是高于还是低于其父节点？是通常更高/更低，还是永远更高/更低。

通常比父节点低。

### 如果决策树过拟合训练集，减少max_depth是否为一个好主意？

可行，可以降低复杂度。

### 如果决策树对训练集欠拟合，尝试缩放输入特征是否为一个好主意？

不可行，决策树不关心特征的缩放。

### 如果在包含100万个实例的训练集上训练决策树需要一个小时，那么在包含1000万个实例的训练集上训练决策树，大概需要多长时间？

决策树复杂度为O(n * mlogm)，如果训练集合大小乘10, m = 10000000，K = n * 10m * log(10m) = 11.7小时

### 如果训练集包含10万个实例，设置presort=True可以加快训练吗？

不可以，presort在数据集合小于千个时候才会加速。
