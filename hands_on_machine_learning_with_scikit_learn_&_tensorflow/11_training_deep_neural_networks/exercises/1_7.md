### 将所有权重初始化为相同的值是否可以

不可以，如果权重相同，任何行为都无法破坏对称性，就会导致形如只有一个神经元，并且很慢。

### 将偏置项初始化为0可行吗？

可以。

### 列举SELU激活函数相比ReLU的三个优点

可以使用负值，输出平均值更容易接近0，可以缓解梯度消失问题。
总是有一个非零的导数。
保证归一化，解决梯度爆炸问题。

### 在那种情况下，你想使用以下每个激活函数：SELU、leaky ReLU、ReLU、tanh、logistic、softmax

SELU是很好的默认选择。
leaky ReLU可以提高速度。
ReLU激活函数简单并且精确输出0的能力是有用的。
需要输出介于-1到1之间可以考虑tanh。
需要评估可能性时候可以考虑logistic。
softmax激活函数在输出层输出互斥类的概率是有效的。

### 如果在使用SGD优化器时将超参数momentum设置的太接近1会发生什么情况

算法的提速会很高，导致在全局最小值震荡，总体会导致收敛速度变慢

### 列举三种能产生稀疏模型的方法

手动设置，l1正则化，模型优化工具箱。

### dropout会减慢训练速度吗？它会减慢推理速度吗？MCDropout呢？

会减慢1/2的训练速度，但对推理速度无影响
MCdropout减慢1/10的预测速度
